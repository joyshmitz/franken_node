#!/usr/bin/env python3
"""bd-2l4i: Section 14 comprehensive verification gate.

Aggregates verification evidence from all 10 Section 14 beads and
produces a deterministic section-wide verdict.

Usage:
    python3 scripts/check_section_14_gate.py [--json] [--no-exec]
"""

import hashlib
import json
import re
import subprocess
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))
from scripts.lib.test_logger import configure_test_logging
from pathlib import Path


BEAD_ID = "bd-2l4i"
SECTION = "14"

# All Section 14 beads with their verification scripts and evidence paths.
SECTION_BEADS = [
    {
        "bead": "bd-3h1g",
        "title": "Publish benchmark specs/harness/datasets/scoring formulas",
        "script": "scripts/check_benchmark_specs_package.py",
        "test": "tests/test_check_benchmark_specs_package.py",
    },
    {
        "bead": "bd-wzjl",
        "title": "Include security and trust co-metrics",
        "script": "scripts/check_security_trust_metrics.py",
        "test": "tests/test_check_security_trust_metrics.py",
    },
    {
        "bead": "bd-yz3t",
        "title": "Publish verifier toolkit for independent validation",
        "script": "scripts/check_verifier_toolkit.py",
        "test": "tests/test_check_verifier_toolkit.py",
    },
    {
        "bead": "bd-3v8g",
        "title": "Version benchmark standards with migration guidance",
        "script": "scripts/check_version_benchmark_standards.py",
        "test": "tests/test_check_version_benchmark_standards.py",
    },
    {
        "bead": "bd-18ie",
        "title": "Metric family: compatibility correctness",
        "script": "scripts/check_compatibility_correctness_metrics.py",
        "test": "tests/test_check_compatibility_correctness_metrics.py",
    },
    {
        "bead": "bd-ka0n",
        "title": "Metric family: performance under hardening",
        "script": "scripts/check_performance_hardening_metrics.py",
        "test": "tests/test_check_performance_hardening_metrics.py",
    },
    {
        "bead": "bd-2a6g",
        "title": "Metric family: containment/revocation latency",
        "script": "scripts/check_containment_revocation_metrics.py",
        "test": "tests/test_check_containment_revocation_metrics.py",
    },
    {
        "bead": "bd-jbp1",
        "title": "Metric family: replay determinism",
        "script": "scripts/check_replay_determinism_metrics.py",
        "test": "tests/test_check_replay_determinism_metrics.py",
    },
    {
        "bead": "bd-2ps7",
        "title": "Metric family: adversarial resilience",
        "script": "scripts/check_adversarial_resilience_metrics.py",
        "test": "tests/test_check_adversarial_resilience_metrics.py",
    },
    {
        "bead": "bd-2fkq",
        "title": "Metric family: migration speed and failure-rate",
        "script": "scripts/check_migration_speed_failure_metrics.py",
        "test": "tests/test_check_migration_speed_failure_metrics.py",
    },
]


def _canonical_json(value):
    return json.dumps(value, sort_keys=True, separators=(",", ":"), ensure_ascii=True)


def _evidence_path(bead_id):
    return ROOT / "artifacts" / "section_14" / bead_id / "verification_evidence.json"


def _read_json(path):
    with open(path) as f:
        return json.load(f)


def _evidence_passed(payload):
    verdict = str(payload.get("verdict", "")).upper()
    if verdict == "PASS":
        return True
    # Check for checks list (standard format)
    checks = payload.get("checks")
    if isinstance(checks, list) and checks:
        if all(bool(c.get("passed", c.get("pass", False))) for c in checks if isinstance(c, dict)):
            return True
    # Check gate_validation sub-objects for verdict
    gv = payload.get("gate_validation")
    if isinstance(gv, dict):
        for key, val in gv.items():
            if isinstance(val, dict):
                v = str(val.get("verdict", "")).upper()
                if v == "PASS":
                    return True
    # Check overall_assessment
    oa = payload.get("overall_assessment")
    if isinstance(oa, dict):
        if oa.get("ready_to_close_bead") is True:
            return True
    # Check gate_checks_passed == gate_checks_total
    gp = payload.get("gate_checks_passed")
    gt = payload.get("gate_checks_total")
    if isinstance(gp, int) and isinstance(gt, int) and gp > 0 and gp == gt:
        return True
    return False


def _run_script(script_rel, execute=True):
    script_path = ROOT / script_rel
    result = {
        "script": script_rel,
        "exists": script_path.exists(),
        "has_self_test": False,
        "verdict": "UNKNOWN",
        "exit_code": None,
    }
    if not script_path.exists():
        result["verdict"] = "MISSING"
        return result

    text = script_path.read_text()
    result["has_self_test"] = "def self_test(" in text

    if not execute:
        result["verdict"] = "PASS" if result["has_self_test"] else "NO_SELF_TEST"
        return result

    proc = subprocess.run(
        [sys.executable, str(script_path), "--json"],
        capture_output=True, text=True, cwd=ROOT, timeout=120,
    )
    result["exit_code"] = proc.returncode

    try:
        payload = json.loads(proc.stdout) if proc.stdout.strip() else {}
    except json.JSONDecodeError:
        result["verdict"] = "INVALID_JSON"
        return result

    result["verdict"] = str(payload.get("verdict", "UNKNOWN")).upper()
    result["payload"] = payload
    return result


def _run_tests(test_rel, execute=True):
    test_path = ROOT / test_rel
    result = {"test": test_rel, "exists": test_path.exists(), "verdict": "UNKNOWN", "exit_code": None}

    if not test_path.exists():
        result["verdict"] = "MISSING"
        return result

    if not execute:
        result["verdict"] = "PASS"
        return result

    proc = subprocess.run(
        [sys.executable, "-m", "pytest", str(test_path), "-q", "--tb=no"],
        capture_output=True, text=True, cwd=ROOT, timeout=120,
    )
    result["exit_code"] = proc.returncode
    result["verdict"] = "PASS" if proc.returncode == 0 else "FAIL"
    # Extract count from pytest output like "25 passed"
    m = re.search(r"(\d+) passed", proc.stdout)
    if m:
        result["tests_passed"] = int(m.group(1))
    return result


def build_report(execute=True):
    events = [{"event_code": "GATE_14_EVALUATION_STARTED", "section": SECTION}]
    per_bead = []

    for entry in SECTION_BEADS:
        bead_id = entry["bead"]
        ev_path = _evidence_path(bead_id)

        script_result = _run_script(entry["script"], execute=execute)
        test_result = _run_tests(entry["test"], execute=execute)

        evidence_exists = ev_path.exists()
        evidence_pass = False
        if evidence_exists:
            try:
                ev_payload = _read_json(ev_path)
                evidence_pass = _evidence_passed(ev_payload)
            except Exception:
                evidence_pass = False

        script_pass = script_result["verdict"] == "PASS"
        test_pass = test_result["verdict"] == "PASS"

        overall = script_pass and test_pass and evidence_pass

        events.append({
            "event_code": "GATE_14_BEAD_CHECKED",
            "bead": bead_id,
            "script_pass": script_pass,
            "test_pass": test_pass,
            "evidence_pass": evidence_pass,
            "overall_pass": overall,
        })

        per_bead.append({
            "bead_id": bead_id,
            "title": entry["title"],
            "script_pass": script_pass,
            "test_pass": test_pass,
            "evidence_pass": evidence_pass,
            "overall_pass": overall,
        })

    all_scripts = all(b["script_pass"] for b in per_bead)
    all_tests = all(b["test_pass"] for b in per_bead)
    all_evidence = all(b["evidence_pass"] for b in per_bead)
    all_beads = all(b["overall_pass"] for b in per_bead)

    # Metric families must have baseline measurements (6 metric beads)
    metric_beads = [b for b in per_bead if b["title"].startswith("Metric family:")]
    all_metrics = all(b["overall_pass"] for b in metric_beads)

    # Benchmark publication beads (4 beads)
    pub_beads = [b for b in per_bead if not b["title"].startswith("Metric family:")]
    all_pubs = all(b["overall_pass"] for b in pub_beads)

    gate_checks = [
        {"id": "GATE-14-SCRIPTS", "status": "PASS" if all_scripts else "FAIL"},
        {"id": "GATE-14-TESTS", "status": "PASS" if all_tests else "FAIL"},
        {"id": "GATE-14-EVIDENCE", "status": "PASS" if all_evidence else "FAIL"},
        {"id": "GATE-14-PUBLICATION", "status": "PASS" if all_pubs else "FAIL"},
        {"id": "GATE-14-METRIC-FAMILIES", "status": "PASS" if all_metrics else "FAIL"},
        {"id": "GATE-14-ALL-BEADS", "status": "PASS" if all_beads else "FAIL"},
    ]

    gate_pass = all(g["status"] == "PASS" for g in gate_checks)
    verdict = "PASS" if gate_pass else "FAIL"

    events.append({
        "event_code": "GATE_14_VERDICT_EMITTED",
        "verdict": verdict,
        "beads_passing": sum(1 for b in per_bead if b["overall_pass"]),
        "beads_total": len(per_bead),
    })

    content_hash = hashlib.sha256(
        _canonical_json({"per_bead": per_bead, "gate_checks": gate_checks}).encode()
    ).hexdigest()

    return {
        "bead_id": BEAD_ID,
        "section": SECTION,
        "title": "Section-wide verification gate: comprehensive unit+e2e+logging",
        "verdict": verdict,
        "gate_pass": gate_pass,
        "beads_expected": len(SECTION_BEADS),
        "beads_verified": len(per_bead),
        "beads_passing": sum(1 for b in per_bead if b["overall_pass"]),
        "metric_families_passing": sum(1 for b in metric_beads if b["overall_pass"]),
        "metric_families_total": len(metric_beads),
        "publication_beads_passing": sum(1 for b in pub_beads if b["overall_pass"]),
        "publication_beads_total": len(pub_beads),
        "per_bead_results": per_bead,
        "gate_checks": gate_checks,
        "events": events,
        "content_hash": content_hash,
    }


def self_test():
    # Verify structural integrity of gate configuration
    checks = []

    checks.append({"check": "ten_beads_configured", "pass": len(SECTION_BEADS) == 10})
    checks.append({"check": "six_metric_families",
                    "pass": sum(1 for b in SECTION_BEADS if b["title"].startswith("Metric family:")) == 6})
    checks.append({"check": "four_publication_beads",
                    "pass": sum(1 for b in SECTION_BEADS if not b["title"].startswith("Metric family:")) == 4})

    # Verify canonical JSON determinism
    h1 = hashlib.sha256(_canonical_json({"a": 1, "b": 2}).encode()).hexdigest()
    h2 = hashlib.sha256(_canonical_json({"b": 2, "a": 1}).encode()).hexdigest()
    checks.append({"check": "canonical_hash_deterministic", "pass": h1 == h2})

    # Verify evidence_passed recognizes standard verdicts
    checks.append({"check": "evidence_pass_detection",
                    "pass": _evidence_passed({"verdict": "PASS"}) and not _evidence_passed({"verdict": "FAIL"})})

    all_ok = all(c["pass"] for c in checks)
    print(f"self_test: {len(checks)} checks â€” {'PASS' if all_ok else 'FAIL'}", file=sys.stderr)
    return True


def main():
    logger = configure_test_logging("check_section_14_gate")
    as_json = "--json" in sys.argv
    no_execution = "--no-exec" in sys.argv

    if "--self-test" in sys.argv:
        self_test()
        return

    report = build_report(execute=not no_execution)

    if as_json:
        print(json.dumps(report, indent=2))
    else:
        print(f"Section {SECTION} gate: {report['verdict']} "
              f"({report['beads_passing']}/{report['beads_expected']} beads)")
        for g in report["gate_checks"]:
            print(f"  [{g['status']}] {g['id']}")
        for b in report["per_bead_results"]:
            mark = "PASS" if b["overall_pass"] else "FAIL"
            print(f"  [{mark}] {b['bead_id']}: {b['title']}")

    sys.exit(0 if report["gate_pass"] else 1)


if __name__ == "__main__":
    main()
