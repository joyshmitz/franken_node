# Bulk Bead Import — franken_node Comprehensive Plan

## Epic: Workspace + Build Infrastructure
- type: task
- priority: 0
- labels: foundation, build
- description: Set up the complete Rust workspace infrastructure for franken_node. This includes: (1) rust-toolchain.toml pinning nightly with edition 2024, (2) workspace-level Cargo.toml with all shared dependencies (anyhow, thiserror, serde, serde_json, tokio, clap, tracing, etc.), (3) release profile optimized for binary size (opt-level=z, lto=true, codegen-units=1, panic=abort, strip=true), (4) #![forbid(unsafe_code)] in all crates, (5) clippy.toml with pedantic+nursery lint levels, (6) rustfmt.toml with project formatting standards, (7) .gitignore for target/ and build artifacts, (8) build.rs with vergen-gix for build metadata embedding. This is the foundational infrastructure that every other task depends on. Without correct workspace setup, nothing else can compile or pass CI. The workspace must support the path dependencies to sibling repos (/dp/franken_engine, /dp/asupersync, /dp/frankentui, /dp/frankensqlite) as specified in section 4 of the plan. RATIONALE: The plan's non-negotiable constraint is that franken_node depends on franken_engine and must not fork engine internals. The workspace topology enforces this architecturally. Edition 2024 and nightly are required per AGENTS.md. ACCEPTANCE: cargo check passes, cargo clippy --all-targets -- -D warnings passes, cargo fmt --check passes, all path deps resolve correctly.

## Epic: Testing Infrastructure Framework
- type: task
- priority: 0
- labels: foundation, testing
- description: Establish the testing infrastructure that all subsequent implementation tasks will use. This includes: (1) Unit test patterns using inline #[cfg(test)] modules per AGENTS.md policy — every module must have tests for happy path, edge cases, and error conditions, (2) Integration test directory structure (tests/conformance/, tests/integration/, tests/security/, tests/perf/, tests/lab/, tests/e2e/, tests/oracle/, tests/harness/), (3) E2E test script skeleton (scripts/e2e_test.sh) with detailed logging, colored output, pass/fail counting, and JSON result export, (4) Test fixture directory structure (fixtures/), (5) Artifact output directory structure (artifacts/), (6) Test helper crate or module with common utilities (temp dirs, fixture loading, assertion helpers, deterministic seed management), (7) Benchmark infrastructure using criterion or similar, (8) Fuzz target directory structure (fuzz/targets/). RATIONALE: The plan requires extensive conformance testing, adversarial fuzzing, deterministic replay testing, and performance benchmarking across all subsystems. Having a well-structured test infrastructure from day one prevents ad-hoc test sprawl and ensures consistent quality gates. The plan explicitly requires artifacts directory for evidence-based claims. ACCEPTANCE: Test directories exist, e2e script skeleton runs and reports 0 tests/0 failures, test helper module compiles, benchmark skeleton runs.

## Epic: CI/CD Pipeline Setup
- type: task
- priority: 0
- labels: foundation, ci
- description: Configure GitHub Actions CI/CD pipeline for franken_node following the patterns from AGENTS.md. Jobs: (1) check — format (cargo fmt --check), clippy (--all-targets -- -D warnings), UBS analysis on changed files, unit tests (cargo nextest run with JUnit XML), (2) coverage — cargo llvm-cov with threshold enforcement (>=70% overall, >=80% for critical modules), upload to Codecov, (3) e2e — end-to-end shell test execution, (4) benchmarks — performance budget checks (push to main only), (5) dependency-direction guard — prevent local engine crate reintroduction (section 10.1), (6) substrate conformance gate — block non-compliant feature merges (section 10.16). Also configure: Dependabot for cargo deps (weekly, Monday 9am, 5 PR limit) and GitHub Actions (weekly, 3 PR limit). RATIONALE: CI enforcement is a non-negotiable constraint from the plan. The split contract between franken_node and franken_engine must be enforced by CI (section 9 Track A exit gate). Without CI, there's no way to enforce the plan's quality and architectural discipline. ACCEPTANCE: All CI jobs run green on the current codebase, dependency guard catches test violations, coverage thresholds are configured.

## Epic: Charter + Split Governance [10.1]
- type: epic
- priority: 1
- labels: governance, charter, track-a
- description: Establish the formal product charter and split governance between franken_node and franken_engine (plan section 10.1). This epic covers 7 deliverables: (1) Add explicit product charter document aligned to franken_engine plan — defines franken_node's ownership of compatibility capture, migration/operator experience, extension ecosystem/trust surfaces, packaging/rollout/enterprise control planes while franken_engine owns native runtime internals/policy semantics/trust primitives. (2) Enforce repository split contract checks in CI — CI must verify that no engine-core crate is reintroduced locally in franken_node. (3) Add dependency-direction guard preventing local engine crate reintroduction — static analysis or CI lint that catches import/dependency violations. (4) Add reproducibility contract templates (env.json, manifest.json, repro.lock) — every claim and benchmark must ship with reproducible artifacts per section 11. (5) Add claim-language policy requiring verifier artifacts for external claims — no public claim without linked reproducible evidence. (6) Add ADR: 'Hybrid Baseline Strategy' codifying spec-first compatibility extraction, not Bun-first clone approach (section 3.3) — this is the foundational architectural decision that legacy code is input to spec/oracle generation, not implementation blueprint. (7) Add implementation-governance policy forbidding line-by-line legacy translation and requiring spec+fixture references in compatibility PRs. RATIONALE: This is the governance foundation. Without explicit split contract and claim-language discipline, the project risks scope confusion, duplicate engine reimplementation, and unverifiable marketing claims. The plan explicitly states that 'No contract, no merge' (section 11). The hybrid baseline strategy decision (section 3.3) is one of the most important architectural choices — choosing spec-first over Bun-clone prevents architecture lock-in and preserves trust-native design. EXIT GATE (Track A): Split contract enforced by CI, deterministic baseline compatibility harness green on seed corpus, initial migration report artifacts reproducible. ACCEPTANCE: Charter document published, CI dependency guard active, reproducibility templates in place, ADR for hybrid baseline strategy written, governance policy documented and linked in PR templates.

## Epic: Compatibility Core [10.2]
- type: epic
- priority: 1
- labels: compatibility, track-b
- description: Implement the core compatibility system that makes franken_node a practical Node/Bun replacement (plan section 10.2). This is critical for adoption — the plan targets >=95% pass rate on targeted compatibility corpus. This epic covers 12 deliverables: (1) Define compatibility bands (core, high-value, edge, unsafe) with policy defaults — each band has different risk/effort profiles and determines migration priority. Core = must-have Node APIs (fs, path, http, process, child_process, events, streams, buffer, url, crypto basics). High-value = heavily-used ecosystem APIs (express patterns, common npm package interfaces). Edge = rarely-used or platform-specific APIs. Unsafe = APIs with inherent security risks that get policy-gated behavior. (2) Implement compatibility behavior registry with typed shim metadata — every compatibility shim is a typed, registered entity with policy visibility. (3) Implement divergence ledger with signed rationale entries — when franken_node intentionally diverges from Node/Bun behavior, the divergence is recorded with cryptographic signature, explicit rationale, policy gate, and remediation guidance. This is one of the 10 impossible-by-default capabilities. (4) Implement compatibility mode selection policy (strict, balanced, legacy-risky) — operators choose their risk appetite, with full traceability. (5) Implement deterministic compatibility fixture runner and result canonicalizer — test harness that runs compatibility fixtures against all three runtimes and produces canonical, comparable results. (6) Implement L1 lockstep runner integration for Node/Bun/franken_node — the product-level oracle that runs identical scenarios across all three runtimes and detects behavioral divergence. This is capability #7 from the impossible-by-default list. (7) Implement minimized divergence fixture generation — when divergences are found, automatically generate minimal reproduction fixtures using delta-debugging. (8) Implement L2 engine-boundary semantic oracle integration policy and release gate linkage — connects product-level compatibility to engine-level semantic integrity. (9) Implement compatibility regression dashboard by API family. (10) Create four-doc spec pack (PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md, EXISTING_NODE_BUN_STRUCTURE.md, PROPOSED_ARCHITECTURE.md, FEATURE_PARITY.md) — these are the Essence Extraction documents from the porting-to-rust methodology (section 5.4). (11) Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling). (12) Add CI gate requiring compatibility implementations to cite spec section + fixture IDs. RATIONALE: Compatibility is the strategic wedge for adoption (section 3.1 doctrine). Without strong compatibility, users have no reason to migrate. But compatibility must be explicit, typed, and policy-visible — not silent behavior drift. The dual-oracle approach (L1 product + L2 engine boundary) ensures both external behavior and internal semantic integrity. The four-doc spec pack follows the porting-to-rust methodology which treats legacy code as input to specification, not implementation blueprint. METHOD STACK ALIGNMENT: extreme-software-optimization (profile shim dispatch overhead), alien-artifact-coding (proof-carrying compatibility claims), alien-graveyard (typed-state transition primitives), porting-to-rust (spec-first essence extraction). EXIT GATE (Track B): Targeted compatibility threshold met, migration pipeline produces actionable reports, divergence receipts generated and reproducible. ACCEPTANCE: Compatibility bands defined and documented, behavior registry operational, divergence ledger recording entries with signatures, lockstep runner executing against all three runtimes, spec pack documents created, CI gate enforcing spec references.

## Epic: Migration System [10.3]
- type: epic
- priority: 1
- labels: migration, track-b
- description: Build the migration autopilot pipeline that converts compatibility into adoption velocity (plan section 10.3). This is initiative #2 from the Top 10 and directly drives the >=3x migration throughput target. 8 deliverables: (1) Build project scanner for API/runtime/dependency risk inventory — scans existing Node/Bun projects to identify all API usage, runtime dependencies, and risk hotspots. Must handle monorepos efficiently (section 9D optimization requirement). (2) Build migration risk scoring model with explainable features — assigns risk scores to each migration finding with transparent feature attribution so teams can prioritize. Uses confidence intervals and hypothesis-tested transformations per alien-artifact methodology (section 9C). (3) Build automated rewrite suggestion engine with rollback plan artifacts — proposes code transformations with rollback receipts. Each suggestion is a hypothesis-tested transformation with confidence intervals. Uses incremental/self-adjusting computation for large projects (section 9B). (4) Build migration validation runner with lockstep checks — validates that rewrites preserve behavior by running lockstep comparisons. (5) Build rollout planner (shadow -> canary -> ramp -> default) per project — staged rollout with confidence gates at each stage. (6) Build migration confidence report with uncertainty bands — quantifies migration risk with explicit uncertainty, not point estimates. (7) Build one-command migration report export for enterprise review — single command that produces a comprehensive, reviewable migration plan. (8) Build deterministic migration failure replay tooling — when migrations fail, failures can be replayed deterministically for root cause analysis. RATIONALE: The plan's core thesis is that 'migration velocity is the growth engine' (section 2). Without easy migration, franken_node can't achieve adoption regardless of how good its security features are. The migration autopilot must make adoption feel 'inevitable, not costly' (section 3.1 doctrine). Deterministic replay for failures is critical because migration failures at scale are expensive and hard to debug. METHOD STACK: extreme-software-optimization (optimize scan/transform throughput with deterministic batching), alien-artifact-coding (confidence intervals and rollback receipts), alien-graveyard (incremental/self-adjusting computation), porting-to-rust (spec-first extraction for compatibility surfaces). ACCEPTANCE: Scanner produces inventory from representative Node projects, risk scores are explainable, rewrite engine generates transformations with rollback artifacts, lockstep validation catches behavioral drift, rollout planner produces staged plans, confidence reports include uncertainty bands, failure replay is deterministic.

## Epic: Extension Ecosystem + Registry [10.4]
- type: epic
- priority: 1
- labels: extensions, registry, trust, track-c
- description: Build the signed extension registry and trust ecosystem (plan section 10.4). This is the foundation for initiatives #3 (Trust Cards), #5 (Policy-Visible Shims), #7 (Secure Distribution), and #9 (Economic Trust Layer). 8 deliverables: (1) Define signed extension package manifest schema — artifact format that embeds capability intent, resource envelopes, remote-effects declarations, and trust policy references as enforceable contracts (section 9K item 4). (2) Define provenance attestation requirements and verification chain — threshold signing, transparency inclusion, provenance attestations, and trusted-builder policies (section 9I item 9). (3) Integrate revocation propagation with canonical freshness checks from 10.13 — revocation is not best-effort but a hard security contract with freshness requirements per safety tier (safe/risky/dangerous). (4) Implement extension trust-card API and CLI surfaces — capability #6 from the impossible-by-default list. Trust cards combine provenance, behavioral telemetry, revocation status, and policy constraints in a single explainable model. Uses authenticated data structures and append-only proofs (section 9B). (5) Implement publisher reputation model with explainable transitions — ecosystem reputation graph with explainable trust transitions (capability #9). Uses decision-theoretic expected-loss for pricing (section 9C). (6) Implement fast quarantine/recall workflow for compromised artifacts — capability #5. Uses anti-entropy reconciliation and bounded degraded-mode semantics (section 9B). (7) Implement extension certification levels tied to policy controls. (8) Implement ecosystem telemetry for trust and adoption metrics. RATIONALE: The plan positions extension trust as THE differentiator over Node/Bun (section 2). Node/Bun have no native trust cards, revocation-aware execution, or quarantine controls. This is where franken_node creates category-level separation. Supply-chain attacks are catastrophic (section 6.1) and existing runtimes only offer external tooling. METHOD STACK: alien-artifact-coding (posterior trust state updates, counterfactual action impacts), alien-graveyard (authenticated data structures, transparency proofs), extreme-software-optimization (optimize trust-card materialization). EXIT GATE (Track C): Trust-native workflows operational in canary environments, replay and audit bundles pass external verifier checks, revocation/quarantine drills meet latency and correctness gates. ACCEPTANCE: Manifest schema defined and validated, provenance chain implemented, revocation freshness enforced, trust cards queryable via API and CLI, reputation model produces explainable scores, quarantine workflow meets latency SLO.

## Epic: Security + Policy Product Surfaces [10.5]
- type: epic
- priority: 1
- labels: security, policy, track-c
- description: Implement the security and policy product surfaces that make trust operational (plan section 10.5). 8 deliverables: (1) Implement policy-visible compatibility gate APIs — any behavior shim is typed, auditable, and policy-gated (initiative #5). Uses policy-as-data signatures and attenuation semantics (section 9B). (2) Implement signed decision receipt export for high-impact actions — every major decision produces a cryptographic receipt (capability #3). (3) Implement deterministic incident replay bundle generation — capability #4, one of the most distinctive features. Includes timeline events, policy decisions, trust artifacts, and all references needed to reproduce high-severity incidents. (4) Implement counterfactual replay mode for policy simulation — 'what if we had used strict policy?' simulation using real incident data. (5) Implement operator copilot action recommendation API — capability #8, live recommended actions with expected-loss rationale (initiative #8). Uses VOI-based ranking (section 9B). (6) Implement expected-loss action scoring with explicit loss matrices — quantified risk-aware decision support (initiative #9 economic trust layer). Uses posterior attacker ROI models (section 9C). (7) Implement degraded-mode policy behavior with mandatory audit events — when systems are partially degraded, behavior is explicit and auditable, not silently weakened. (8) Implement policy change approval workflows with cryptographic audit trail. RATIONALE: Security controls that are 'decorative' provide no real value (section 1 design philosophy #2). These surfaces make security operationally measurable. The expected-loss approach (section 5.2 alien-artifact methodology) converts trust from intuition into evidence. Deterministic replay is foundational for the 100% replay artifact availability target. METHOD STACK: alien-artifact-coding (expected-loss vectors, uncertainty bands, confidence contexts), alien-graveyard (VOI-based ranking), extreme-software-optimization (optimize recommendation latency). ACCEPTANCE: Compatibility gates enforce policy, decision receipts are cryptographically signed, incident bundles replay deterministically, counterfactual mode simulates alternative policies, copilot recommendations include expected-loss rationale, degraded mode emits audit events.

## Epic: Performance + Packaging [10.6]
- type: epic
- priority: 2
- labels: performance, packaging, track-d
- description: Build the performance infrastructure and packaging system (plan section 10.6). Performance is a product feature, not vanity metric (section 7). 7 deliverables: (1) Build product-level benchmark suite with secure-extension scenarios — benchmarks must include security overhead, not just raw speed. Co-metrics include containment latency, revocation convergence, replay determinism (section 14). (2) Add cold-start and p99 latency gates for core workflows — low startup overhead for migration/CI loops, predictable p99 under extension churn (section 7.1). (3) Optimize lockstep harness throughput and memory — streaming normalization, parallel fixture evaluation (section 9D). (4) Optimize migration scanner throughput for large monorepos — deterministic batching, cache reuse (section 9D). (5) Add packaging profiles for local/dev/enterprise deployments. (6) Add artifact signing and checksum verification for releases — Sigstore signing following dcg pattern from AGENTS.md release process. (7) Add release rollback bundles with deterministic restore checks. RATIONALE: Performance must be measured holistically — security overhead is part of the performance story. The plan requires p50/p95/p99 baselines, profile artifacts, before/after tables, and tail-latency notes for security instrumentation (section 7.3). Packaging must support the install-to-first-safe-run pathway. METHOD STACK: extreme-software-optimization (mandatory baseline->profile->prove->implement->verify loop). ACCEPTANCE: Benchmark suite runs with secure-extension scenarios, cold-start meets target, p99 gates enforced, packaging profiles work for all deployment modes, releases are signed.

## Epic: Conformance + Verification [10.7]
- type: epic
- priority: 2
- labels: conformance, verification, track-d
- description: Build the conformance and verification infrastructure (plan section 10.7). This supports the verifier economy — external verification as default for claims (section 9H program #3). 6 deliverables: (1) Build compatibility golden corpus and fixture metadata schema — normative test corpus that defines what compatibility means, with versioned fixtures. (2) Adopt canonical trust protocol vectors from 10.13+10.14 and enforce release/publication gates — golden vectors for serialization, signatures, control-channel frames. (3) Add fuzz/adversarial tests for migration and shim logic — decode-DoS, replay/splice scenarios (section 10.13 item on adversarial fuzz corpus). (4) Add metamorphic tests for compatibility invariants — test that compatibility properties hold under transformation. (5) Add verifier CLI conformance contract tests. (6) Add external-reproduction playbook and automation scripts — enables independent external replication of headline claims (section 13 target: >=2 independent reproductions). RATIONALE: The plan requires statistical rigor (confidence intervals, reproducibility guarantees, verifier receipts) for headline claims (section 9C item 10). The verifier economy is a strategic moat — transparent proof workflows are hard for competitors to copy (section 9K item 12). Without conformance infrastructure, all the other work is claims without evidence. ACCEPTANCE: Golden corpus exists and passes, trust protocol vectors validate, fuzz targets running, metamorphic tests catching invariant violations, verifier CLI passes conformance, external reproduction playbook tested.

## Epic: Operational Readiness [10.8]
- type: epic
- priority: 2
- labels: operations, fleet, track-d
- description: Build the operational infrastructure for fleet-scale deployment (plan section 10.8). 6 deliverables: (1) Implement fleet control API for quarantine/revocation operations — fleet quarantine UX (initiative #6). Turns engine-level containment into operator-grade workflows with global scope, blast-radius views, convergence indicators, and rollback controls. (2) Adopt canonical structured observability + stable error taxonomy from 10.13 — stable metric names, error codes, trace correlation IDs, machine-readable recovery hints (section 9I item 18). (3) Implement deterministic safe-mode startup and operation flags. (4) Implement incident bundle retention and export policy — retention classes (required vs ephemeral) with explicit persistence obligations (section 9I item 16). (5) Implement operator runbooks for high-severity trust incidents — detection signature, immediate containment, replay procedure, rollback procedure. (6) Implement disaster-recovery drills for control-plane failures. RATIONALE: Operational readiness is the difference between a demo and a production system. The plan targets friction-minimized install-to-first-safe-production pathway. Fleet quarantine with bounded convergence guarantees (capability #5) requires robust operational infrastructure. METHOD STACK: extreme-software-optimization (optimize propagation latency, conflict reconciliation), alien-graveyard (anti-entropy reconciliation, bounded degraded-mode semantics). ACCEPTANCE: Fleet API operational, error taxonomy stable, safe-mode works, incident bundles export correctly, runbooks tested, DR drills pass.

## Epic: Moonshot Disruption Track [10.9]
- type: epic
- priority: 3
- labels: moonshot, disruption, track-e
- description: Build moonshot-level capabilities that define the category (plan section 10.9, drawing from section 9F). 6 deliverables: (1) Build public Node/Bun/franken_node benchmark campaign infrastructure — defines and owns the benchmark standard for secure extension runtimes (initiative #10). (2) Build autonomous adversarial campaign runner with continuous updates — live exploit market simulator for policy stress-testing (moonshot #3). (3) Build migration singularity demo pipeline for flagship repositories — showcase that migration can be near-automatic for important projects (moonshot #1). (4) Build verifier economy portal and external attestation publishing flow — make external verification easy and default (frontier program #3). (5) Build trust economics dashboard with attacker-ROI deltas — quantify attack-cost amplification (initiative #9). (6) Build category-shift reporting pipeline with reproducible artifacts. RATIONALE: These are the capabilities that make franken_node categorically different from Node/Bun. Without them, it's 'a better Node clone' which violates the category-creation doctrine (section 3.1). The plan explicitly states: 'If users can get the same outcomes with a thin wrapper around Node/Bun defaults, the feature is insufficient.' EXIT GATE (Track E): Multiple impossible-by-default capabilities adopted in production, category benchmark adoption outside project core team, sustained red-team delta and migration velocity advantages. ACCEPTANCE: Benchmark campaigns running, adversarial runner operating, migration singularity demonstrated, verifier portal live, economics dashboard published.

## Epic: FCP-Inspired Hardening + Interop [10.10]
- type: epic
- priority: 2
- labels: hardening, interop, fcp
- description: Implement FCP-spec-inspired product hardening and interoperability features (plan section 10.10, drawing from section 9E). 11 deliverables: (1) Define canonical product trust object IDs with domain separation — object identity discipline for product trust artifacts (9E item 1). (2) Enforce deterministic serialization and signature preimage rules from 10.13+10.14 — canonical serialization contracts for operator receipts (9E item 2). (3) Implement policy checkpoint chain for product release channels — checkpointed policy frontier for rollback resistance (9E item 3). (4) Implement rollback/fork detection using canonical divergence and marker proofs from 10.14 — detect state divergence in control-plane propagation. (5) Implement audience-bound token chains for control actions — capability token delegation chains (9E item 4). (6) Implement key-role separation for control-plane signing/encryption/issuance — key-role separation and owner-signed attestations (9E item 5). (7) Integrate session-authenticated control channel + anti-replay framing from 10.13 — secure control traffic (9E item 6). (8) Integrate revocation freshness semantics from 10.13 before risky/dangerous product actions — freshness gates (9E item 7). (9) Implement zone/tenant trust segmentation policies — zone-style trust boundaries (9E item 8). (10) Adopt stable error namespace and compatibility policy from 10.13 — stable observability (9E item 9). (11) Adopt canonical trust protocol vectors/golden fixtures from 10.13+10.14 as product publication and release gates — interop stability (9E item 10). RATIONALE: FCP-spec patterns provide production-hardened security primitives. These are not novel research — they're proven patterns adapted for franken_node's product layer. Domain-separated object IDs prevent confused-deputy attacks. Deterministic serialization enables reproducible verification. Epoch-scoped validity prevents split-policy ambiguity. ACCEPTANCE: Object IDs have domain separation, serialization is canonical, policy checkpoint chain works, token delegation implemented, key roles separated, control channel authenticated, freshness gates enforced, zone segmentation operational.

## Epic: FrankenSQLite-Inspired Runtime Systems [10.11]
- type: epic
- priority: 2
- labels: runtime, frankensqlite-inspired
- description: Implement FrankenSQLite-inspired runtime system patterns (plan section 10.11, drawing from section 9G). 14 deliverables: (1) Define capability profiles for product subsystems and enforce narrowing — capability-context-first APIs (9G item 1). (2) Add ambient-authority audit gate for security-critical modules — no ambient network/spawn/time effects in restricted modules. (3) Add checkpoint-placement contract in all long orchestration loops. (4) Adopt cancel->drain->finalize protocol from 10.15 for product services — strict cancellation protocol (9G item 2). (5) Implement bounded masking helper for tiny atomic operations. (6) Adopt obligation-tracked two-phase channel contracts from 10.15 for critical flows — reserve/commit style for high-impact side effects (9G item 3). (7) Implement supervision tree with restart budgets and escalation policies. (8) Adopt deterministic lab runtime and protocol scenario suites from 10.14+10.15 — deterministic testing (9G item 4). (9) Implement BOCPD regime detector for workload/incident stream shifts — Bayesian Online Changepoint Detection for regime shifts. (10) Implement VOI-budgeted monitor scheduling for expensive diagnostics — Value of Information based scheduling. (11) Integrate monotonic security epochs and transition barriers from 10.14. (12) Integrate remote idempotency + saga semantics from 10.14. (13) Integrate scheduler lane and global bulkhead policies from 10.14+10.15. (14) Implement anti-entropy reconciliation for distributed trust state — O(delta) reconciliation with proof-carrying recovery artifacts (9G item 10). RATIONALE: These runtime system patterns are what make franken_node's control plane production-grade. Without proper cancellation, obligation tracking, capability narrowing, and anti-entropy reconciliation, the sophisticated trust and security features would be brittle under real-world conditions. ACCEPTANCE: Capability profiles enforced, ambient authority blocked in restricted modules, cancellation protocol operational, supervision tree working, lab runtime scenarios passing, epoch management integrated.

## Epic: Frontier Programs Execution [10.12]
- type: epic
- priority: 2
- labels: frontier, track-e
- description: Execute the 5 adopted frontier programs at production reliability (plan section 10.12, drawing from section 9H). 7 deliverables: (1) Define migration singularity artifact contract and verifier format — machine-checked migration plans with rollback receipts (frontier program #1). (2) Implement end-to-end migration singularity pipeline for pilot cohorts. (3) Implement trust fabric convergence protocol and degraded-mode semantics — signed trust artifacts, revocation-first execution, rapid global containment (frontier program #2). (4) Implement verifier-economy SDK with independent validation workflows — external verification as default (frontier program #3). (5) Implement operator intelligence recommendation engine with rollback proofs — expected-loss-aware control recommendations (frontier program #4). (6) Implement ecosystem network-effect APIs (registry/reputation/compliance evidence) — registry + reputation + compliance evidence loops (frontier program #5). (7) Add frontier demo gates with external reproducibility requirements. RATIONALE: The frontier programs are what convert franken_node from a good runtime into a category-defining platform. Each program compounds on the others: migration singularity drives adoption, trust fabric provides security differentiation, verifier economy builds external credibility, operator intelligence makes operations safer, ecosystem effects create lock-in. EXIT GATE: Multiple impossible-by-default capabilities adopted in production, category benchmark outside core team. ACCEPTANCE: Migration singularity demonstrated on pilot projects, trust fabric operational, verifier SDK published and used externally, operator recommendations include expected-loss, ecosystem APIs serving real data.

## Epic: Connector Lifecycle + State Management [10.13a]
- type: epic
- priority: 1
- labels: connector, lifecycle, state, fcp-deep
- description: Implement the connector lifecycle state machine and state management system (plan section 10.13, items 1-9). In this context, 'connector/provider' means extension integration class per section 8.8 terminology contract. DELIVERABLES: (1) Connector lifecycle enum + transition table + illegal-transition rejection tests — strict FSM (discovered->verified->installed->configured->active) with explicit non-happy-path states (failed, paused, stopped). All transitions have invariants. Artifacts: connector_lifecycle.md, connector_lifecycle_transitions.rs, lifecycle_transition_matrix.json. (2) Lifecycle-aware health gating + rollout-state persistence — activation requires lifecycle+health gate; rollout state survives restart/failover; recovery replay reproduces same state. Artifacts: rollout_state_machine.md, lifecycle_health_gate.rs. (3) Standard connector method contract validator (handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown) — versioned schemas, machine-readable contract reports. Artifacts: connector_method_validator.rs, connector_method_contract.md. (4) Connector protocol conformance harness — CI gate fails publication for non-conformant connectors; bypass requires explicit policy override. Artifacts: connector_protocol_harness.rs, connector-conformance.yml. (5) Canonical connector state root/object model — explicit state model tagging (stateless, singleton_writer, crdt); local cache divergence detectable/repairable. Artifacts: connector_state_model.md, connector_state_persistence.rs. (6) Singleton-writer fencing validation (lease_seq + lease-object linkage) — unfenced/stale-fenced writes rejected; monotonic fence checks. Artifacts: singleton_writer_fencing.rs, fencing_rules.md. (7) CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) — merge laws covered by fixtures; deterministic merge output; schema tags prevent type confusion. Artifacts: crdt_merge_fixtures.rs, fixtures/crdt/*.json. (8) Snapshot policy (every_updates, every_bytes) + bounded replay targets — replay cost bounded by thresholds; snapshots validated against chain heads. Artifacts: state_snapshot_policy.md, state_replay_bound.rs. (9) State schema version contracts + deterministic migration hints — version transitions require declared migration path; migrations idempotent and replay-stable. Artifacts: state_schema_migrations.md, state_migration_contract.rs. RATIONALE: The connector lifecycle state machine is foundational because incidents become mechanically diagnosable — operators can reason about exact state and legal next actions. Externalized state (item 5) is the foundation for deterministic operations, auditable replay, and multi-node continuity. Lease fencing (item 6) makes duplicate side effects structurally impossible. CRDTs (item 7) enable multi-node state without coordination. ACCEPTANCE: All FSM transitions covered by tests, illegal transitions rejected, health gating operational, conformance harness blocking non-conformant connectors, state persistence working with CRDT merge, fencing enforcement working, schema migrations idempotent.

## Epic: Sandbox + Network Security [10.13b]
- type: epic
- priority: 1
- labels: sandbox, network, security, fcp-deep
- description: Implement sandbox profiles and network security controls (plan section 10.13, items 10-13). DELIVERABLES: (1) Sandbox profile system (strict, strict_plus, moderate, permissive) with policy compiler — profile compiler emits enforceable low-level policy for each tier; profile downgrade blocked by policy; selection auditable. Artifacts: sandbox_policy_compiler.rs, sandbox_profiles.md. (2) Strict-plus isolation backend (microVM when available, hardened fallback) — maps to microVM isolation where supported; unsupported platforms use hardened fallback with equivalent policy guarantees. Artifacts: strict_plus_backend_matrix.md, strict_plus_isolation.rs. (3) Network Guard egress layer with HTTP+TCP policy enforcement + audit emission — all connector egress traverses guard path; allow/deny matches policy semantics; every decision emits structured audit event. This is the highest-leverage runtime defense against compromised extension behavior (section 9I item 7). Artifacts: network_guard.rs, network_guard_policy.rs. (4) SSRF-deny default policy template — localhost/tailnet/private CIDR denied unless explicitly allowed; regression tests cover common SSRF patterns. Artifacts: network_guard_default.toml, ssrf_default_deny.rs. RATIONALE: Security posture must be configurable without being vague (section 9I item 6). A tiered sandbox model prevents lowest-common-denominator security. Centralized egress control (Network Guard) is the highest-leverage defense against SSRF and lateral movement. These are not optional decorations — they're runtime defaults with measurable behavior (section 1 design philosophy #2). ACCEPTANCE: Sandbox profiles compile and enforce correct policies, strict_plus isolation works on supported platforms, network guard intercepts all egress, SSRF default deny blocks private CIDR.

## Epic: Supply Chain Trust Infrastructure [10.13c]
- type: epic
- priority: 1
- labels: supply-chain, trust, signatures, fcp-deep
- description: Implement supply chain trust verification infrastructure (plan section 10.13, items 14-18). DELIVERABLES: (1) Fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks) — unsupported major versions and missing required features hard-fail activation; negotiation decisions logged. Artifacts: manifest_negotiation.md, manifest_negotiation_fail_closed.rs. (2) Domain-separated interface-hash verification + admission failure telemetry — interface hash uses domain-separated derivation; invalid hashes block admission. Artifacts: interface_hash.rs, interface_hash_verification.rs. (3) Threshold signature verification for connector publication artifacts — publication requires configured threshold quorum; partial signature sets rejected. Artifacts: threshold_signatures.md, threshold_signature_verification.rs. (4) Transparency-log inclusion proof checks in install/update pipelines — install/update fails if required inclusion proof is missing/invalid; verification replayable. Artifacts: transparency_verifier.rs, transparency_inclusion.rs. (5) Provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders) — policy engine enforces required attestations and builder trust constraints. Artifacts: provenance_policy.md, attestation_gate.rs. RATIONALE: Simple signature validity is necessary but insufficient for supply-chain trust (section 9I item 9). Robust trust requires multi-signal verification: threshold signing prevents single-key compromise, transparency logs provide public auditability, provenance attestations verify build integrity. Fail-closed negotiation (section 9I item 8) surfaces incompatibility at install/activation time instead of runtime. ACCEPTANCE: Manifest negotiation rejects incompatible connectors, interface hashes verified with domain separation, threshold signatures enforced, transparency proofs checked, provenance gates operational.

## Epic: Activation Pipeline + Revocation [10.13d]
- type: epic
- priority: 1
- labels: activation, revocation, crash-loop, fcp-deep
- description: Implement the deterministic activation pipeline, crash-loop protection, and revocation freshness system (plan section 10.13, items 19-23). DELIVERABLES: (1) Deterministic activation pipeline: sandbox->ephemeral secret mount->capability issue->health-ready transition — stage order fixed and enforced; partial activation cannot leak persistent secrets; restart replay reproduces identical transcript. Artifacts: activation_pipeline.md, activation_pipeline_determinism.rs. (2) Crash-loop detector with automatic rollback + known-good pin fallback — configurable thresholds; rollback to known-good is automatic and auditable; rollback cannot bypass trust policy. This gives operators safe self-healing under bad releases (section 9I item 10). Artifacts: crash_loop_detector.rs, crash_loop_rollback.rs. (3) Revocation registry with monotonic revocation-head checkpoints — heads monotonic per zone/tenant; stale head updates rejected; head state recoverable from canonical storage. Artifacts: revocation_registry.md, revocation_head_monotonicity.rs. (4) Revocation freshness enforcement per safety tier (safe/risky/dangerous) — stale-frontier risky/dangerous actions denied; overrides follow policy and are receipt-backed. Freshness semantics turn revocation from best-effort signal into hard security contract (section 9I item 11). Artifacts: revocation_freshness_gate.rs, safety_tier_freshness.md. (5) Degraded-mode audit events for stale revocation frontier overrides — every override emits required audit schema fields. Artifacts: degraded_mode_audit_events.rs, degraded_mode_audit_schema.md. RATIONALE: Deterministic activation (section 9I item 10) reduces MTTR and prevents cascading instability. Crash-loop rollback provides safe self-healing. The revocation freshness system is the critical link between trust infrastructure and runtime behavior — without it, trust is advisory rather than enforced. Three-tier freshness (safe/risky/dangerous) allows offline operation to remain practical while never pretending trust state is fresher than it is. ACCEPTANCE: Activation pipeline stages execute in fixed order, crash-loop detection and rollback working, revocation heads are monotonic, freshness gates deny stale-frontier risky/dangerous actions, degraded-mode overrides emit audit events.

## Epic: Lease Service + Execution Planner [10.13e]
- type: epic
- priority: 2
- labels: leases, placement, offline, fcp-deep
- description: Implement generic lease service, device-aware execution planning, and offline capability infrastructure (plan section 10.13, items 24-32). DELIVERABLES: (1) Generic lease service for operation execution, state writes, migration handoff — shared semantics across purposes; deterministic expiry/renewal; stale lease usage rejected. Artifacts: lease_service.rs, generic_leases.md. (2) Deterministic lease coordinator selection + quorum signature verification — coordinator selection deterministic for identical inputs; quorum requirements vary by safety tier. Artifacts: lease_coordinator_selection.rs, lease_quorum_rules.md. (3) Overlapping-lease conflict policy + deterministic fork handling logs — dangerous conflicts halt and alert; fork logs contain reproducible evidence. Artifacts: lease_conflict_policy.md, overlapping_lease_conflicts.rs. (4) Device profile registry + placement policy schema — validated schema with freshness checks; placement policies reject invalid constraints. Artifacts: device_profile_schema.md, placement_policy_schema.rs. (5) Execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers — stable output for identical inputs; explainable factor weights. Artifacts: execution_scorer.rs, execution_planner_determinism.rs. (6) Predictive pre-staging engine for high-probability offline artifacts — raises offline coverage; budget limits prevent prefetch storms. Artifacts: predictive_prestaging.md, prestaging_coverage_improvement.rs. (7) Offline coverage tracker + SLO dashboards — coverage metrics computed continuously; SLO breach alerts trigger automatically. 'Works offline' becomes measurable commitment (section 9I item 14). Artifacts: offline_slo_metrics.md, offline_coverage_metrics.rs. (8) Background repair controller with bounded work-per-cycle + fairness controls — respects per-cycle work caps and fairness constraints. Artifacts: background_repair_controller.rs, repair_fairness.rs. RATIONALE: Leases are the minimal distributed primitive for deterministic ownership (section 9I item 12). Device-aware placement is required for predictable fleet behavior at scale (section 9I item 13). Quantified offline SLOs create product differentiation (section 9I item 14). ACCEPTANCE: Lease service operational with all required semantics, coordinator selection deterministic, conflict handling robust, device placement working, offline coverage tracked and SLOs reported.

## Epic: Admission + Quarantine Controls [10.13f]
- type: epic
- priority: 2
- labels: admission, quarantine, fcp-deep
- description: Implement admission control and quarantine-by-default pipeline (plan section 10.13, items 33-37). DELIVERABLES: (1) Per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu) — all budget dimensions enforced; limit breaches rate-limited and logged; budgets tunable without code changes. Artifacts: admission_budget_model.md, per_peer_budget_enforcement.rs. (2) Anti-amplification response bounds for retrieval/sync traffic + adversarial harness — response payloads never exceed request-declared bounds under adversarial inputs; unauthenticated limits stricter. Artifacts: anti_amplification_harness.rs, anti_amplification_rules.md. (3) Quarantine-by-default store for unreferenced objects with quota+TTL enforcement — unknown objects enter quarantine; quota and TTL eviction enforce hard caps; quarantined objects excluded from primary state. Artifacts: quarantine_store.rs, quarantine_retention.rs. (4) Schema-gated quarantine promotion rules + promotion provenance receipts — promotion requires reachability/auth/pin plus schema validation; invalid promotions fail closed. Artifacts: quarantine_promotion_rules.md, quarantine_promotion_gate.rs. (5) Control-plane retention policy (required vs ephemeral) + storage enforcement — required artifacts durably stored; ephemeral droppable under policy. Artifacts: control_plane_retention.md, retention_class_enforcement.rs. RATIONALE: Early admission gating is cheaper and safer than downstream cleanup after malicious amplification (section 9I item 15). Quarantine-first treatment prevents storage pollution and DoS. Retention classing aligns audit completeness with cost discipline. ACCEPTANCE: Admission budgets enforced, anti-amplification bounds respected, quarantine store operational with quota/TTL, promotion gates working, retention policy enforced.

## Epic: Control Channel + Telemetry [10.13g]
- type: epic
- priority: 2
- labels: protocol, telemetry, errors, fcp-deep
- description: Implement authenticated control channel, stable telemetry, and conformance infrastructure (plan section 10.13, items 38-45). DELIVERABLES: (1) Authenticated control channel with per-direction sequence monotonicity + replay-window checks — rejects out-of-window/non-monotonic frames; replay attacks blocked. Artifacts: control_channel.rs, control_channel_replay_window.rs. (2) Bounded parser/resource-accounting guardrails on control-channel frame decode — byte/CPU/allocation ceilings; oversized/malformed frames fail fast. Artifacts: control_channel_parser_limits.md, parser_budget_guardrails.rs. (3) Stable telemetry namespace for protocol/capability/egress/security planes — metric names/labels versioned and frozen; schema validator enforces rules. Artifacts: telemetry_namespace.md, metric_schema_stability.rs. (4) Stable error code namespace + machine-readable retryable/retry_after/recovery_hint contract — unique namespaced codes; recovery fields for all non-fatal errors. Artifacts: error_code_contract.md, error_contract_stability.rs. (5) Distributed trace correlation IDs across connector execution + control-plane — all high-impact flows carry trace fields end-to-end. Artifacts: trace_correlation_end_to_end.rs, trace_context_contract.md. (6) MVP vs Full conformance profile matrix + publication claim rules — maps required capabilities to claim language; unsupported claims blocked. Artifacts: profile_matrix.md, profile_claim_gate.rs. (7) Mandatory serialization/object-id/signature/revocation/source-diversity interop suites — covers all mandatory classes across implementations. Artifacts: tests/interop/*.rs, fixtures/interop/*.json. (8) Adversarial fuzz corpus gates (decode-DoS, replay/splice handshake) — CI gate enforces minimum fuzz health budget. Artifacts: fuzz/targets/*, adversarial_fuzzing.md. (9) Formal schema spec files + golden vectors for serialization, signatures, control-channel frames — normative schemas and vectors; verification CLI passes full suite. Artifacts: spec/FNODE_TRUST_SCHEMA_V1.cddl, vectors/fnode_trust_vectors_v1.json. RATIONALE: Control-plane integrity is non-negotiable (section 9I item 17). Stable telemetry prevents automation regression (section 9I item 18). Formal schemas and golden vectors enable third-party verifier implementations (section 9I item 20). ACCEPTANCE: Control channel rejects replay attacks, parser guardrails enforced, telemetry namespace stable, error codes machine-readable, trace correlation working end-to-end, conformance profiles defined, interop suites passing, fuzz targets running, golden vectors published.

## Epic: Evidence Ledger System [10.14a]
- type: epic
- priority: 1
- labels: evidence, ledger, frankensqlite-deep
- description: Implement the deterministic evidence ledger for policy-influenced decisions (plan section 10.14, items 1-5). This is inspired by FrankenSQLite's evidence-ledger concept — the missing bridge between advanced policy automation and production trust (section 9J item 1). DELIVERABLES: (1) Define EvidenceEntry schema for product control decisions — covers decision kind, candidates, constraints, chosen action, witness references; field ordering canonical; schema validation enforced in CI. Artifacts: evidence_entry_schema.md, evidence_entry_v1.json. (2) Bounded evidence ledger ring buffer + lab spill-to-artifacts mode — production memory stays bounded; lab mode writes full spill artifacts for failing scenarios. Artifacts: evidence_ledger.rs, evidence_ledger_bounds.rs. (3) Require evidence emission for policy-driven commit/abort/quarantine/release actions — all policy-driven decisions emit mandatory evidence entries; missing entry causes conformance failure. Artifacts: policy_decision_evidence.rs, policy_evidence_requirements.md. (4) Attach trace-witness references to high-impact ledger entries — stable trace witness IDs resolve in replay bundles; broken references fail integrity check. Artifacts: evidence_trace_witness_linking.rs, witness_reference_contract.md. (5) Evidence-ledger replay validator — deterministically replays recorded decision contexts; mismatches reported with minimal diff. Artifacts: evidence_replay_validator.rs. RATIONALE: Explainable autonomy — operators can audit WHY the system acted, not just what happened. Evidence ledger entries with trace witnesses enable deterministic replay of any policy decision. This is fundamental to the 100% replay artifact availability target. Without evidence, trust is opaque. ACCEPTANCE: Evidence schema validated in CI, ledger bounded in production, all policy decisions emit evidence entries, witness references resolve in replay, replay validator reproduces decisions deterministically.

## Epic: Correctness + Policy Boundaries [10.14b]
- type: epic
- priority: 1
- labels: correctness, policy, guardrails, frankensqlite-deep
- description: Implement the correctness/policy separation and statistical control model (plan section 10.14, items 6-11). DELIVERABLES: (1) Define immutable correctness envelope that controllers cannot modify — enumerates non-tunable invariants; controller API rejects writes outside allowed set. Artifacts: correctness_envelope.md, controller_envelope_enforcement.rs. (2) Controller boundary checks rejecting correctness-semantic mutations — pre-apply validation; violation attempts return stable error class; audit records rejected intent. Artifacts: controller_boundary_checks.rs, controller_mutation_rejection.rs. (3) Anytime-valid guardrail monitor set for security/durability-critical budgets — always-on for critical budgets; outputs valid under optional stopping; alert thresholds policy-configurable. Artifacts: anytime_valid_guardrails.md, anytime_guardrail_monitors.rs. (4) Bayesian posterior diagnostics for explainable policy ranking — posterior metrics surfaced for diagnostics; updates reproducible from stored observations. Artifacts: bayesian_diagnostics.rs, bayesian_policy_ranking.rs. (5) Guardrail precedence enforcement: anytime-valid bounds override Bayesian recommendations — decision engine always checks guardrail before recommendation apply. Artifacts: guardrail_precedence.rs, decision_precedence_rules.md. (6) Policy action explainer distinguishing diagnostic confidence from guarantee confidence — separate sections for heuristic vs guarantee confidence; UI/API expose both. Artifacts: policy_explainer_contract.md, policy_explainer_output.rs. RATIONALE: Hard separation between correctness invariants and tunable policy (section 9J item 2) prevents 'smart controller drift' from turning tuning into hidden regressions. Dual-statistics model (Bayesian for ranking + anytime-valid for guarantees, section 9J item 3) provides both fast adaptive policy AND mathematically defensible guarantees. Guardrail precedence is non-negotiable: safety bounds must always override optimization recommendations. ACCEPTANCE: Correctness envelope defined and enforced, controller mutations rejected, guardrails always-on, Bayesian diagnostics operational, guardrail precedence verified, explainer output separates diagnostic from guarantee confidence.

## Epic: Monotonic Hardening System [10.14c]
- type: epic
- priority: 2
- labels: hardening, monotonic, frankensqlite-deep
- description: Implement monotonic hardening autopilot that self-defends under stress (plan section 10.14, items 12-18). DELIVERABLES: (1) Monotonic hardening mode state machine with one-way escalation — transitions monotonic unless explicit governance rollback artifact; state transitions durable/replayable. Artifacts: hardening_state_machine.rs, monotonic_hardening.rs. (2) Automatic hardening trigger on guardrail rejection evidence — triggers hardening within configured latency; idempotent; includes causal evidence pointer. Artifacts: hardening_auto_trigger.rs, hardening_trigger_policy.md. (3) Overhead/rate clamp policy for hardening escalations — min/max bounds; deterministic calculations; clamp hits visible in telemetry. Artifacts: hardening_clamps.rs, hardening_clamp_bounds.rs. (4) Retroactive hardening pipeline — appends additional protection artifacts without rewriting canonical objects; identity remains stable. Artifacts: retroactive_hardening.md, retroactive_hardening_union_only.rs. (5) Integrity sweep escalation/de-escalation driven by evidence trajectories — cadence adjusts per evidence bands; hysteresis prevents oscillation. Artifacts: integrity_sweep_scheduler.rs, integrity_sweep_adaptation.rs. (6) 'Durability contract violated' diagnostic bundles — includes causal event sequence, failed artifacts, proof context; bundle deterministic. Artifacts: durability_contract_violated.md, durability_violation_bundle.rs. (7) Gate durable-claiming operations on verifiable marker/proof availability — claims fail closed when verification incomplete. Artifacts: durable_claim_gate.rs, durable_claim_requirements.md. RATIONALE: Monotonic safety pressure is a strong anti-fragility property (section 9J item 4). The system self-defends under stress instead of dithering around static thresholds. Retroactive hardening adds protection without disrupting canonical objects. This directly supports category-defining security claims. ACCEPTANCE: Hardening state machine is monotonic, automatic triggers fire on guardrail rejections, clamps respect bounds, retroactive hardening is union-only, sweep cadence adapts, violation bundles are deterministic.

## Epic: Proof-Carrying + Deterministic Operations [10.14d]
- type: epic
- priority: 2
- labels: proofs, determinism, encoding, frankensqlite-deep
- description: Implement proof-carrying operations and deterministic encoding infrastructure (plan section 10.14, items 19-26). DELIVERABLES: (1) Proof-carrying repair artifacts in decode/reconstruction paths — repair operations emit proof metadata; verification API validates. Artifacts: proof_carrying_decode.rs, proof_carrying_repair.rs. (2) Suspicious-artifact challenge flow requiring proofs before trust promotion — challenge defers promotion pending proof; unresolved challenges timeout to deny. Artifacts: suspicious_artifact_challenge.md, challenge_flow_before_promotion.rs. (3) Proof-presence requirement for quarantine promotion in high-assurance modes — fails without proof bundle in high-assurance mode. Artifacts: high_assurance_quarantine_promotion.rs, high_assurance_promotion.md. (4) Content-derived deterministic seed derivation for encoding/repair schedules — domain-separated, stable; identical content/config produces identical schedule. Artifacts: deterministic_seed.rs, deterministic_seed_derivation.rs. (5) Determinism conformance tests (multi-replica fixture runs yield byte-identical artifacts). Artifacts: replica_artifact_determinism.rs, fixtures/determinism/*. (6) Object-class profile registry (critical marker, trust receipt, replay bundle, telemetry) — required classes with default policies. Artifacts: object_class_profiles.md, object_class_profiles.toml. (7) Per-class symbol-size/overhead/fetch policy with benchmark-derived defaults. Artifacts: object_class_tuning.rs, benchmarks/object_class_tuning/*. (8) Profile tuning harness + signed benchmark-driven policy updates. Artifacts: profile_tuning_harness.rs, policy_update_signing.md. RATIONALE: Proof-carrying operations convert opaque repair heuristics into auditable, reproducible trust decisions (section 9J item 5). Deterministic regeneration is a compounding resilience lever (section 9J item 6). Object-class-specific policies avoid one-size performance penalties (section 9J item 7). ACCEPTANCE: Proofs emitted and verified in repair paths, challenge flow working, high-assurance mode enforces proof-presence, seed derivation deterministic, replicas produce identical artifacts, object classes registered with policies.

## Epic: Tiered Trust Storage [10.14e]
- type: epic
- priority: 2
- labels: storage, tiering, durability, frankensqlite-deep
- description: Implement tiered trust artifact storage with explicit durability modes (plan section 10.14, items 27-31). DELIVERABLES: (1) L1/L2/L3 trust artifact storage abstraction with source-of-truth designation — tier abstraction exposes clear authority boundaries; source-of-truth immutable by class; recovery reconstructs derived tiers. Artifacts: tiered_trust_storage.md, tiered_storage_recovery.rs. (2) durability=local and durability=quorum(M) semantics — mode semantics enforced end-to-end; mode switches auditable/policy-gated. Artifacts: durability_modes.md, durability_mode_semantics.rs. (3) Retrievability-before-eviction proofs for L2->L3 transitions — eviction requires successful proof check; failed proofs block eviction. Artifacts: retrievability_gate.rs, retrievability_before_eviction.rs. (4) Cancel-safe eviction saga (upload->verify->retire) with deterministic compensations — no partial retire on cancellation/crash; compensation deterministic; zero orphan states. Artifacts: eviction_saga.md, eviction_saga_cancel_safety.rs. (5) Require RemoteCap for all network-bound trust/control operations — fail without capability token; centralized auditable checks. Artifacts: remote_cap_enforcement.rs, remote_cap_contract.md. RATIONALE: Durable long-horizon evidence is a strategic moat (section 9J item 8). Explicit durability tiers (section 9J item 9) align runtime behavior with business risk tolerance. Retrievability-before-eviction prevents accidental evidence loss. Cancel-safe sagas prevent partial state corruption. ACCEPTANCE: Tiered storage abstraction working, durability modes enforced, eviction proofs required, sagas cancel-safe, RemoteCap enforced for network operations.

## Epic: Remote Effects + Distributed Control [10.14f]
- type: epic
- priority: 2
- labels: remote, bulkhead, scheduling, frankensqlite-deep
- description: Implement remote effects contracts, bulkhead isolation, and lane-aware scheduling (plan section 10.14, items 32-38). DELIVERABLES: (1) Named remote computation registry — accepts only registered computation names; unknown/malformed names rejected with stable codes; registry versioned. Artifacts: computation_registry.rs, remote_name_registry.rs. (2) Idempotency key derivation from request bytes with epoch binding — deterministic, domain-separated, epoch-bound; collisions negligible; vectors published. Artifacts: idempotency.rs, idempotency_key_derivation.rs. (3) Idempotency dedupe store semantics — same key/same payload deduped; same-key different-payload conflicts hard-fail; restart recovery handled. Artifacts: idempotency_dedupe_store.rs, idempotency_store_semantics.md. (4) Global remote bulkhead with configurable max_in_flight + overload backpressure — operations never exceed cap; deterministic backpressure; p99 latency within target. Artifacts: remote_bulkhead.rs, remote_bulkhead_under_load.rs. (5) Lane-aware scheduler classes with priority policies — task classes mapped to lanes by policy; starvation/misclassification checks; lane telemetry exposed. Artifacts: lane_mapping_policy.md, lane_mapping_enforcement.rs. (6) Monotonic control epoch in canonical manifest state — regressions rejected; epoch changes produce signed events. Artifacts: control_epoch_contract.md, control_epoch_monotonicity.rs. (7) Fail-closed validity window check rejecting future-epoch artifacts. Artifacts: future_epoch_rejection.rs, validity_window_rules.md. RATIONALE: Remote side effects are the biggest hidden complexity in distributed control (section 9J item 10). Strict contracts (named, idempotent, saga-safe) are mandatory. Bulkhead isolation prevents retry-storm self-DoS (section 9J item 11). Lane-aware scheduling converts scheduler internals into product-grade SLA guarantees (section 9J item 20). Epoch discipline is the clean mechanism for safe rolling transitions (section 9J items 12-13). ACCEPTANCE: Named registry operational, idempotency enforced with dedup, bulkhead capping in-flight, lanes correctly prioritized, epochs monotonic, future-epoch artifacts rejected.

## Epic: Epoch Management + Marker Streams [10.14g]
- type: epic
- priority: 2
- labels: epochs, markers, mmr, frankensqlite-deep
- description: Implement epoch transition barriers, marker streams, and cryptographic history proofs (plan section 10.14, items 39-47). DELIVERABLES: (1) Epoch-scoped key derivation for trust artifact authentication — binds to epoch+domain; cross-epoch reuse impossible by construction. Artifacts: epoch_scoped_keys.rs, epoch_key_derivation.rs. (2) Epoch transition barrier protocol across core services with drain requirements — requires participant drain acks; commits only on full success; timeout aborts safely. Artifacts: epoch_barrier_protocol.md, epoch_transition_barrier.rs. (3) Transition abort semantics on timeout/cancellation unless explicit force policy — default aborts; force policy explicit/scoped/audited. Artifacts: epoch_transition_abort_semantics.rs, force_transition_policy.md. (4) Append-only marker stream for high-impact control events with dense sequence + hash-chain invariants — torn-tail recovery deterministic; invariant breaks trigger hard alert. Artifacts: marker_stream.rs, marker_stream_invariants.rs. (5) O(1) marker lookup by sequence + O(log N) timestamp-to-sequence search. Artifacts: marker_lookup_complexity.rs, marker_lookup_algorithms.md. (6) Fork/divergence detection via marker-id prefix comparison + binary search — greatest common prefix deterministic; logarithmic scaling. Artifacts: marker_divergence_detection.rs, divergence_detection.md. (7) Optional MMR checkpoints + inclusion/prefix proof APIs for external verifiers — enables/disables without corrupting marker truth; proof APIs verify claims. Artifacts: mmr_proofs.rs, mmr_proof_verification.rs. (8) Root pointer atomic publication protocol (write temp->fsync temp->rename->fsync dir) — survives crash-injection; root switch atomic. Artifacts: root_publication_protocol.md, root_pointer_crash_safety.rs. (9) Root-auth fail-closed bootstrap checks — rejects unauthenticated/malformed root pointers. Artifacts: root_bootstrap_fail_closed.rs, root_bootstrap_auth.md. RATIONALE: Epoch barriers prevent high-impact actions straddling incompatible trust epochs (section 9J item 13). The marker stream is the atomic truth of control history (section 9J item 14). MMR proofs enable scalable verifier-economy workflows (section 9J item 15). Single mutable root pointer simplifies failure recovery (section 9J item 16). ACCEPTANCE: Epoch key derivation working, barriers enforcing drain before transition, marker stream append-only with hash chains, lookups meeting complexity targets, divergence detection working, MMR proofs verifiable, root pointer crash-safe, bootstrap fail-closed.

## Epic: Lab + Verification Infrastructure [10.14h]
- type: epic
- priority: 2
- labels: lab, testing, verification, frankensqlite-deep
- description: Implement the deterministic lab, verification, and conformance infrastructure (plan section 10.14, items 48-53). DELIVERABLES: (1) Deterministic repro bundle export for control-plane failures/incidents — includes seed, config, event-sequence trace, evidence references; replay deterministic; schema versioned. Artifacts: repro_bundle_export.rs, repro_bundle_replay.rs. (2) Virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing — deterministic fault schedules from seed; covers drop/reorder/corrupt classes. Artifacts: virtual_transport_faults.rs, virtual_transport_harness.md. (3) Cancellation injection at all await points for critical control workflows — leak-free and half-commit-free invariants hold under injected cancellations. Artifacts: cancellation_injection_control_workflows.rs, cancel_injection_matrix.md. (4) DPOR-style schedule exploration gates for control/epoch/remote protocols — covers targeted protocol classes; minimal counterexample traces on failure; runs within bounded CI budget. Artifacts: dpor_protocol_exploration.rs, dpor_gate_scope.md. (5) Conformance suite for ledger determinism, idempotency, epoch validity, marker/MMR proof correctness — normative fixtures for all four domains; required for release profile claim. Artifacts: fsqlite_inspired_suite.rs, fixtures/conformance/fsqlite_inspired/*. RATIONALE: Deterministic fault labs are a prerequisite for confidence in ambitious distributed control logic (section 9J item 18). Cancel correctness is core infrastructure quality, not a nicety (section 9J item 19). DPOR-style exploration catches scheduling-dependent bugs that standard testing misses. Conformance suites convert quality claims into machine-checkable evidence. ACCEPTANCE: Repro bundles replay deterministically, fault harness reproduces protocol failures, cancellation injection finds no leaks, DPOR explorer operates within CI budget, conformance suite passes all normative fixtures.

## Epic: Asupersync Ownership + Boundaries [10.15a]
- type: epic
- priority: 1
- labels: asupersync, governance, boundaries
- description: Establish asupersync integration governance and boundary enforcement (plan section 10.15, items 1-4). Asupersync is the correctness/control kernel in the three-kernel architecture (section 8.4). DELIVERABLES: (1) Tri-kernel ownership contract (franken_engine, asupersync, franken_node) — names owners for execution, correctness, and product planes; boundary violations have deterministic CI failures; exceptions require signed waiver. Artifacts: tri_kernel_ownership_contract.md, ownership_boundary_checks.rs. (2) High-impact workflow inventory mapped to required asupersync primitives — every critical workflow mapped to Cx, region, cancellation, obligation, remote, epoch, evidence requirements. Artifacts: high_impact_workflow_map.md, workflow_primitive_matrix.json. (3) Cx-first signature policy for control-plane async entrypoints — lint/gate rejects new high-impact async APIs missing &Cx; existing exceptions enumerated and time-bounded. Artifacts: cx_first_policy.rs, cx_first_api_gate.rs. (4) Ambient-authority audit gate for control-plane modules — ambient network/spawn/time effects in restricted modules fail CI; allowlist explicit and signed. Artifacts: ambient_authority_gate.rs, ambient_authority_policy.md. RATIONALE: The plan's hard runtime invariants (section 8.5) require asupersync integration. Without formal ownership boundaries, the three-kernel architecture would drift into monolithic confusion. Cx-first APIs ensure all high-impact operations are capability-controlled. Ambient-authority audit prevents security blind spots. ACCEPTANCE: Ownership contract published, workflow inventory complete, Cx-first policy enforced in CI, ambient authority gate active.

## Epic: Control-Plane Execution Migration [10.15b]
- type: epic
- priority: 1
- labels: asupersync, control-plane, migration
- description: Migrate control-plane execution to asupersync-backed region-owned execution (plan section 10.15, items 5-8). DELIVERABLES: (1) Migrate lifecycle/rollout orchestration to region-owned execution trees — lifecycle runs under region ownership; region close implies quiescence. Artifacts: region_owned_lifecycle.rs, region_tree_topology.md. (2) Implement request->drain->finalize cancellation protocol across high-impact workflows — transitions explicit/deterministic; cleanup budget bounds documented/tested. Artifacts: cancellation_protocol_contract.md, cancel_drain_finalize.rs. (3) Replace critical ad hoc messaging with obligation-tracked two-phase channels — publish/revoke/quarantine/migration critical paths use reserve/commit; leak oracle green under cancellation. Artifacts: obligation_tracked_channels.rs, two_phase_effects.md. (4) Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready) — every control task class has lane assignment and budget policy; starvation checks automated. Artifacts: control_lane_mapping.md, control_lane_policy.rs. RATIONALE: Hard runtime invariants 1-5 (section 8.5) require Cx-first, region-owned, cancel-correct, obligation-tracked, lane-disciplined execution. Ad hoc detached task patterns are explicitly off-charter. This migration is AS-B (section 8.6). EXIT GATE (AS-B): Migrated critical workflows pass deterministic replay tests, no detached-orphan paths on critical flows, no obligation leaks. ACCEPTANCE: Lifecycle runs in regions, cancellation protocol operational, two-phase channels replacing ad hoc messaging, lane mapping enforced.

## Epic: Asupersync Remote + Evidence Integration [10.15c]
- type: epic
- priority: 2
- labels: asupersync, remote, evidence, sagas
- description: Integrate asupersync remote, saga, epoch, and evidence semantics into the control plane (plan section 10.15, items 9-16). DELIVERABLES: (1) Integrate canonical remote named-computation registry from 10.14 for distributed control. (2) Enforce canonical idempotency-key contracts from 10.14 on retryable remote requests. (3) Add saga wrappers with deterministic compensations for multi-step workflows — cancellation/crash at any step leaves 'never happened' or committed state. (4) Integrate canonical epoch-scoped validity windows from 10.14. (5) Integrate canonical epoch transition barriers from 10.14. (6) Make canonical evidence-ledger emission from 10.14 mandatory for policy-influenced decisions. (7) Integrate canonical evidence replay validator from 10.14 into decision gates. (8) Integrate deterministic lab runtime scenarios for high-impact control protocols. RATIONALE: This is AS-C (section 8.6) — distributed control semantics. Remote effects must be capability-gated, named, idempotent, and saga-safe (hard runtime invariant 6). Epoch discipline (invariant 7) and evidence-by-default (invariant 8) complete the asupersync integration. EXIT GATE (AS-C): Degraded remote dependencies preserve p99/control liveness, epoch transitions deterministic/auditable, idempotency/saga conformance green. ACCEPTANCE: Remote registry adopted, idempotency enforced, sagas operational, epoch validity/barriers integrated, evidence emission mandatory, replay validator integrated, lab scenarios passing.

## Epic: Asupersync Lab + Release Gates [10.15d]
- type: epic
- priority: 2
- labels: asupersync, lab, release-gates
- description: Enforce asupersync verification, lab testing, release gates, and observability (plan section 10.15, items 17-25). DELIVERABLES: (1) Enforce canonical cancellation injection gate for critical control workflows. (2) Enforce canonical virtual transport fault harness for distributed protocols. (3) Enforce canonical DPOR-style schedule exploration for epoch/lease/remote/evidence. (4) Add release gate requiring asupersync-backed conformance on high-impact features — blocks claims/features lacking required conformance artifacts. (5) Add observability dashboards for region health, obligation health, lane pressure, cancel latency. (6) Add invariant-breach runbooks for region-quiescence, obligation leak, cancel-timeout incidents. (7) Add migration plan for existing non-asupersync control surfaces with burn-down tracking. (8) Add performance budget guard for integration overhead in hot paths. (9) Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence. RATIONALE: This is AS-D and AS-E (section 8.6) — evidence/verification industrialization and production hardening. Deterministic protocol verification gates are hard runtime invariant 9. No ambient authority (invariant 10) is enforced here. EXIT GATES: AS-D: Policy decision replay determinism proven, deterministic fault scenarios reproducible, protocol gate suite required in release. AS-E: Controller cannot mutate correctness envelope, all claims backed by evidence + conformance, runbooks cover breach/containment. ACCEPTANCE: Cancellation injection passing, fault harness operational, DPOR explorer within budget, release gate blocking unverified claims, dashboards live, runbooks tested, migration burn-down tracked, overhead within budget.

## Epic: Adjacent Substrate Integration [10.16]
- type: epic
- priority: 2
- labels: substrate, frankentui, frankensqlite, fastapi
- description: Integrate adjacent substrate repositories (frankentui, frankensqlite, sqlmodel_rust, fastapi_rust) per section 10.16. 17 deliverables grouped: POLICY+ARCHITECTURE: (1) Publish substrate policy contract defining mandatory/should-use scopes, exceptions, waiver process. (2) Add architecture dependency map showing where each substrate is required. FRANKENTUI: (3) Define integration contract for console/TUI surfaces. (4) Migrate relevant TUI workflows to frankentui primitives. (5) Add deterministic visual/snapshot and interaction tests. FRANKENSQLITE: (6) Define persistence integration contract for control/audit/replay state. (7) Implement adapter layer for required persistence surfaces. (8) Add migration path from interim stores. SQLMODEL: (9) Define usage policy and typed model boundaries. (10) Integrate in domains where typed schema/query safety is high-EV. FASTAPI: (11) Define control-plane service integration contract. (12) Build service skeleton for operator/verifier/fleet-control endpoints. CROSS-SUBSTRATE: (13) Add contract tests validating end-to-end behavior. (14) Add substrate conformance gate in CI. (15) Add waiver workflow for justified exceptions. (16) Add performance overhead guardrails. (17) Add claim-language gate for substrate-backed evidence. RATIONALE: The plan mandates specific substrate choices (section 4 and section 8.7). Substrate choice is policy, not preference. We don't chase novelty by reimplementing mature capabilities. Waiver discipline ensures exceptions are bounded and justified. ACCEPTANCE: All substrate contracts published, TUI migrated, persistence adapter working, typed models integrated, API skeleton operational, CI gates enforcing compliance, waivers tracked.

## Epic: Radical Expansion - Speculative Execution + Security [10.17a]
- type: epic
- priority: 3
- labels: radical, speculation, adversary, frontier
- description: Implement proof-carrying speculative execution and Bayesian adversary graph (plan section 10.17, items 1-2). DELIVERABLES: (1) Proof-carrying speculative execution governance for extension-host hot paths — transforms cannot activate without proof receipts; guard failure degrades to deterministic safe baseline; activation only via approved franken_engine interfaces. Artifacts: proof_carrying_speculation.md, proof_executor.rs, proof_speculation_guards.rs. (2) Bayesian adversary graph + automated quarantine controller — risk posterior updates deterministic from identical evidence; policy thresholds trigger reproducible control actions (throttle/isolate/revoke/quarantine) with signed evidence. Artifacts: adversary_graph.rs, quarantine_controller.rs, bayesian_risk_quarantine.rs. RATIONALE: Proof-carrying speculation creates the rare combination of faster-than-baseline under normal load AND never-silently-wrong under edge conditions (section 9K item 1). The adversary graph converts defense from isolated fortress mode into probabilistic, graph-structured risk management (section 9K item 2). ACCEPTANCE: Speculative transforms require proofs, guard failures degrade safely, adversary graph updates deterministically, quarantine triggers are reproducible.

## Epic: Radical Expansion - Time-Travel + Artifacts [10.17b]
- type: epic
- priority: 3
- labels: radical, replay, artifacts, frontier
- description: Implement time-travel runtime, capability-carrying artifacts, and isolation mesh (plan section 10.17, items 3-5). DELIVERABLES: (1) Deterministic time-travel runtime capture/replay — captured executions replay byte-for-byte equivalent; includes stepwise state navigation and divergence explanation. Artifacts: time_travel_runtime.md, time_travel_engine.rs. (2) Capability-carrying extension artifact format — admission fails closed on invalid capability contracts; runtime enforcement matches admitted envelope. Artifacts: capability_artifact_format.md, artifact_contract.rs. (3) Adaptive multi-rail isolation mesh with hot-elevation policy — workloads can be promoted to stricter rails without losing policy continuity. Artifacts: isolation_mesh.md, isolation_rail_router.rs. RATIONALE: Time-travel replay shifts root-cause analysis from guesswork to deterministic forensics (section 9K item 3). Capability-carrying artifacts make trust mechanically enforceable (item 4). Adaptive isolation provides strong containment for risky code while preserving low latency for trusted workloads (item 5). ACCEPTANCE: Time-travel replay is deterministic, capability artifacts enforce contracts, isolation mesh supports hot-elevation.

## Epic: Radical Expansion - Advanced Security [10.17c]
- type: epic
- priority: 3
- labels: radical, zk, oracle, staking, frontier
- description: Implement ZK attestation, semantic oracle, staking, governor, and firewall (plan section 10.17, items 6-11). DELIVERABLES: (1) Zero-knowledge attestation for selective compliance verification — verifiers validate predicates without privileged disclosure. (2) L2 engine-boundary N-version semantic oracle — differential harness blocks release on high-risk divergences. (3) Security staking/slashing framework for publisher trust governance — high-risk capabilities enforce stake gates; malicious behavior triggers deterministic slashing. (4) Self-evolving optimization governor with safety-envelope enforcement — shadow evaluation + anytime-valid safety checks; unsafe policies auto-reject/revert. (5) Intent-aware remote effects firewall — classifies by inferred intent category; risky intents trigger challenge/simulate/deny/quarantine. (6) Information-flow lineage and exfiltration sentinel — sensitive lineage tags persist; covert exfiltration detected above recall/precision thresholds. RATIONALE: These are the most ambitious security capabilities. ZK attestation maximizes verifiability while minimizing data leakage (item 6). The semantic oracle prevents silent compatibility erosion (item 7). Staking aligns economic incentives (item 8). The governor enables continuous optimization without risk (item 9). Intent-aware firewall closes semantic blind spots (item 10). ACCEPTANCE: ZK proofs verified, oracle catching divergences, staking operational, governor within safety envelope, firewall classifying intents, sentinel detecting exfiltration.

## Epic: Radical Expansion - Verifier SDK + Claims [10.17d]
- type: epic
- priority: 3
- labels: radical, verifier, claims, frontier
- description: Implement verifier SDK, hardware planner, counterfactual lab, and claim compiler (plan section 10.17, items 12-15). DELIVERABLES: (1) Universal verifier SDK + replay capsule format — external verifiers can replay signed capsules independently; APIs stable/versioned. (2) Heterogeneous hardware planner with policy-evidenced placements — maps workloads across hardware while preserving deterministic policy. (3) Counterfactual incident lab + mitigation synthesis — replays real incidents with synthesized mitigations and expected-loss deltas. (4) Claim compiler + public trust scoreboard — claims must compile to executable evidence contracts; unverifiable text blocked. RATIONALE: The verifier economy is a strategic moat (section 9K item 12). Hardware awareness extracts performance gains (item 13). Counterfactual lab shifts incident response from reactive to engineered (item 14). Claim compiler makes truth operational (item 15). ACCEPTANCE: SDK replays capsules externally, hardware planner operational, counterfactual lab produces expected-loss deltas, claim compiler blocks unverifiable claims.

## Epic: Verifiable Execution Fabric (VEF) [10.18]
- type: epic
- priority: 3
- labels: vef, proofs, compliance, frontier
- description: Implement the Verifiable Execution Fabric — proof-carrying runtime compliance (plan section 10.18, section 9L). 13 deliverables: (1) VEF policy-constraint language + compiler. (2) Canonical ExecutionReceipt schema + deterministic serialization. (3) Hash-chained receipt stream with periodic commitment checkpoints. (4) Receipt-window selection + proof-job scheduler with latency budgets. (5) Proof-generation service interface (backend-agnostic). (6) Proof-verification gate API for trust decisions. (7) VEF integration into high-risk control transitions. (8) Degraded-mode policy for proof lag/outage. (9) VEF evidence in verifier SDK replay capsules. (10) VEF coverage in claim compiler + scoreboard. (11) Adversarial test suite (tampering, replay, stale-policy, mismatch). (12) Performance budget gates for p95/p99 overhead. (13) Release gate requiring VEF evidence for designated claims. RATIONALE: VEF upgrades trust from 'we logged and replayed it' to 'we can cryptographically demonstrate policy compliance' — a major category-level differentiation. It fuses evidence ledger, replay, verifier SDK, and claim compiler into a single verifiable loop. This is the highest-EV accretive layer (section 9L rationale). ACCEPTANCE: Policy constraints compile to proof predicates, receipts chained with commitments, proofs generated and verified, degraded modes auditable, adversarial tests passing, overhead within budget.

## Epic: Adversarial Trust Commons (ATC) [10.19]
- type: epic
- priority: 3
- labels: atc, federation, privacy, frontier
- description: Implement the Adversarial Trust Commons — privacy-preserving federated behavioral intelligence (plan section 10.19, section 9M). 13 deliverables: (1) ATC federation trust model + participant identity contracts. (2) Canonical federated signal schema. (3) Local signal extraction from trust cards/adversary graph/events. (4) Privacy envelope (secure aggregation + differential-privacy budget). (5) Mergeable sketch system for scalable pattern sharing. (6) Poisoning-resilient aggregation + outlier-robust priors. (7) Sybil-resistant participation controls. (8) Contribution-weighted intelligence access + reciprocity. (9) ATC global priors in Bayesian adversary graph. (10) Privacy-preserving urgent routing for revocation/quarantine. (11) Verifier APIs for federation computations. (12) Degraded/offline modes + local-first fallback. (13) Release gate requiring ATC evidence for ecosystem claims. RATIONALE: ATC transforms defense from isolated fortress into collective network defense. High-quality detections in one environment improve prevention in all others while preserving privacy. This is structurally hard for incumbent runtimes to replicate (section 9M rationale). ACCEPTANCE: Federation operational, privacy budgets enforced, poisoning resistance validated, Sybil controls working, global priors consumed by adversary graph, urgent routing meeting SLO.

## Epic: Dependency Graph Immune System (DGIS) [10.20]
- type: epic
- priority: 3
- labels: dgis, topology, supply-chain, frontier
- description: Implement the Dependency Graph Immune System — topological contagion modeling and preemptive containment (plan section 10.20, section 9N). 15 deliverables: (1) Canonical dependency/topology graph schema. (2) Deterministic graph ingestion pipeline. (3) Topological risk metric engine (fan-out, betweenness, articulation, percolation). (4) Maintainer/publisher fragility model + SPOF detector. (5) Adversarial contagion simulator (xz-style, multi-stage campaigns). (6) Critical-node immunization planner + choke-point barriers. (7) Trust barrier primitives + policy wiring. (8) DGIS context in trust cards + adversary graph. (9) Graph-aware quarantine + rollback orchestration. (10) ATC interop for topology indicators. (11) Expected-loss cascade economics + ROI ranking. (12) Operator copilot for dependency updates. (13) DGIS health in migration gates. (14) Adversarial validation suite. (15) Performance budgets + release claim gates. RATIONALE: DGIS changes security economics from reactive node-level defense to proactive graph-level immunization, shrinking blast radius before attackers exploit high-leverage positions. Makes xz-style campaigns materially harder (section 9N rationale). ACCEPTANCE: Graph ingestion working, topology metrics computed, fragility model detecting SPOFs, contagion simulator running, immunization planner producing barrier sets, trust cards showing topology context.

## Epic: Behavioral Phenotype Evolution Tracker (BPET) [10.21]
- type: epic
- priority: 3
- labels: bpet, evolution, temporal, frontier
- description: Implement the Behavioral Phenotype Evolution Tracker — longitudinal trust genetics for extensions (plan section 10.21, section 9O). 16 deliverables: (1) Canonical BehavioralGenome schema + version-lineage contract. (2) Deterministic phenotype feature extraction per version. (3) Signed lineage graph builder. (4) Cohort-aware baseline modeling. (5) Temporal drift feature engine (velocity, acceleration, entropy, capability-creep gradient). (6) Changepoint + regime-shift detection (Bayesian + HMM). (7) Survival/hazard model for compromise-propensity. (8) Unified evolution-risk scorer with explainability. (9) BPET in trust cards + adversary graph. (10) BPET + DGIS topology-amplified early warning. (11) BPET + ATC federated temporal intelligence. (12) Evolution stability in migration gates. (13) BPET in economic trust layer + operator copilot. (14) Adversarial evaluation suite (slow-roll mimicry, staged camouflage). (15) BPET governance (thresholds, appeals, overrides). (16) Performance budgets + release claim gates. RATIONALE: BPET shifts detection from reactive mutation catching to predictive compromise precursors, allowing containment before malicious payloads are fully expressed. Adds the missing time dimension that converts a strong trust stack into a predictive trust stack (section 9O rationale). ACCEPTANCE: Genome schema defined, phenotype extraction deterministic, lineage graph queryable, baselines calibrated, drift features stable, regime shifts detected, hazard model calibrated, risk scorer explainable.

## Comprehensive Unit Test Suite
- type: task
- priority: 1
- labels: testing, quality
- description: Implement comprehensive unit tests for every module following AGENTS.md testing policy. Every module must include inline #[cfg(test)] unit tests covering: (1) Happy path — normal operation produces expected results, (2) Edge cases — empty inputs, max values, boundary conditions, unicode handling, large payloads, (3) Error conditions — invalid inputs, missing dependencies, permission failures, timeout scenarios. Test naming convention: test_<module>_<scenario>_<expected_outcome>. Minimum test counts per subsystem: compatibility registry (20+), divergence ledger (15+), migration scanner (20+), trust cards (15+), policy engine (25+), evidence ledger (15+), lockstep runner (20+), sandbox profiles (10+), network guard (15+), revocation system (15+), lease service (10+), marker stream (10+), epoch management (10+). Tests must be deterministic — no flaky tests, no timing dependencies, no network calls. Use seed-based random generation for property tests. All tests must pass with cargo test and rch exec -- cargo test. Coverage target: >=70% overall, >=80% for critical modules (evaluator, policy engine, evidence ledger, revocation system). RATIONALE: The plan requires extensive testing at every level. Without comprehensive unit tests, the sophisticated features (evidence ledger, deterministic replay, proof-carrying operations) cannot be trusted. AGENTS.md explicitly requires tests for happy path, edge cases, and error conditions. ACCEPTANCE: >=70% coverage overall, >=80% for critical modules, all tests deterministic, no cargo test failures.

## E2E Test Scripts + Logging Infrastructure
- type: task
- priority: 1
- labels: testing, e2e, logging
- description: Implement comprehensive end-to-end test scripts with detailed logging. Following the dcg pattern from AGENTS.md (scripts/e2e_test.sh with 120+ tests), build scripts/e2e_test.sh for franken_node covering: (1) CLI command smoke tests — every CLI subcommand (init, run, migrate audit, migrate rewrite, migrate validate, verify lockstep, trust card, trust list, trust revoke, trust quarantine, fleet status, fleet release, incident bundle, incident replay, incident counterfactual, registry publish, registry search, bench run, doctor) runs without crash and produces expected output format. (2) Migration pipeline integration — end-to-end migration from audit through rewrite through validation on sample Node projects. (3) Trust workflow integration — extension registration, trust card generation, revocation, quarantine, and release cycle. (4) Compatibility lockstep — run lockstep fixtures against multiple runtimes and verify divergence reporting. (5) Incident replay — capture an event sequence, export bundle, replay, and verify deterministic reproduction. (6) Policy enforcement — verify that policy gates (strict, balanced, legacy-risky) produce expected allow/deny decisions. LOGGING REQUIREMENTS: Colored output with pass/fail indicators, test count/pass/fail summary, JSON result export (e2e_output.json), timestamp for each test, detailed failure messages with expected vs actual, verbose mode (--verbose) showing all output, exit code 0 only if all tests pass. Script structure: helper functions for assertions, temp directory management, fixture setup/teardown, timeout enforcement per test. RATIONALE: E2E tests catch integration issues that unit tests miss. Detailed logging enables fast debugging when tests fail in CI. The JSON output enables automated trend tracking. ACCEPTANCE: All CLI commands have smoke tests, migration pipeline e2e passes, trust workflow e2e passes, lockstep e2e passes, replay e2e passes, JSON results exported, verbose mode works.
